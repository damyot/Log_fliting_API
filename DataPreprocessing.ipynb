{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e79c8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install all the required packages and run the required libraries\n",
    "##-----------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skimpy import clean_columns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a3c0706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List of Functions\n",
    "##-----------------------------------------------\n",
    "#write to CSV file\n",
    "#writeCSV(table DataSet, String file):The writeCSV function writes the dataframe to a CSV file.\n",
    "#Arguments: DataSet is the name of dataframe to save, file is the name of the CSV output file.\n",
    "#Returns a CSV file\n",
    "def writeCSV(dataset,filename):\n",
    "     dataset.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a65c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a CSV file\n",
    "#readCSV(String file):The readCSV function reads the CSV file into workplace\n",
    "#Arguments: file is the CSV file in the select work directory.\n",
    "# def readCSV(filename,time,format=\"%d/%m/%Y %H:%M\"):\n",
    "def readCSV(filename,time):\n",
    "    csv_data = pd.read_csv(filename)\n",
    "    cleanAllHeaders(csv_data)\n",
    "    csv_data=addNewTime(csv_data,cleanText(time),\"%d/%m/%Y %H:%M\")\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54541ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(colName):\n",
    "    cleanedText=re.sub(\"[$&+,:;=?@#|'<>.^*()%!-]\",\"_\",colName).lower()\n",
    "    return cleanedText;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2973af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select dataset columns for analysis\n",
    "#selectColumns(table DataSet, string columnName, …): This function selects/keeps the list columns needed for analysis from the dataset. Only the\n",
    "#list of selected columns/attributes are included in the dataset. \n",
    "#Arguments: DataSet is the name of the dataframe, columnName is the name of the column to keep in the dataset. Many can be listed, separated by commas.\n",
    "#Returns a dataset including only the list of columns/attributes that are selected\n",
    "def selectColumns(dataset,selectCol):\n",
    "    return dataset[selectCol]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "837db080",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete columns from the dataset\n",
    "def deleteColumns(dataset,deleteCol):\n",
    "    return dataset.drop(columns=deleteCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71195d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanOneHeader(dataset,col_name):\n",
    "    cleaned=cleanText(col_name)\n",
    "    dataset=dataset.rename(columns={col_name:cleaned})\n",
    "    return dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22237f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean column headers\n",
    "#cleanHeaders(table DataSet)\n",
    "#This function cleans the headers of the columns from spaces and other special characters.\n",
    "#It only keeps lower case letters, numbers, and underscores (_). The spaces are replaced by ‘_’ and the special characters are removed. \n",
    "#Returns a dataframe with clean header names\n",
    "def cleanAllHeaders(dataset):\n",
    "    col_list=dataset.columns.tolist()\n",
    "    for i in col_list:\n",
    "        cleaned=cleanText(i)\n",
    "        dataset=dataset.rename(columns={i:cleaned})\n",
    "    return dataset;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a66018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter rows\n",
    "#The filter function keeps records/rows based on the conditions specified. \n",
    "#Only the rows where the condition is TRUE are kept in the DataSet. \n",
    "#The filter function supports multiple functions, for example: ==, >, <, >=, <=, &, | , ! . \n",
    "def filterRows(dataset,conditions):\n",
    "    return dataset.query(conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ee462b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##remove rows with low frequency\n",
    "#Column represents the column you want to filter\n",
    "# freq the threshold value that is used to filter out rows whose count is less than freq.\n",
    "def removeEventsLowFrequency(dataset,event,freq):\n",
    "    return dataset[dataset.groupby(event)[event].transform('count').ge(freq)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a17336e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete traces with number of events less than a specific number (num)\n",
    "def deleteTraceLengthLessThan(dataset,company_id,num):\n",
    "    return dataset.groupby(company_id).filter(lambda x : len(x)>=num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "263ba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete traces that do not start with one of many start events\n",
    "# Not Sort \n",
    "def deleteTruncatedTracesStart(dataset,company_id,event,start_events):\n",
    "    return data.groupby(company_id).filter(lambda oneCompanyData: oneCompanyData.iloc[0][event] in start_events)\n",
    "\n",
    "## multiple conditions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d99ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete traces that do not start with one of many start events\n",
    "# Need to Sort \n",
    "def deleteTruncatedTracesStartSort(dataset,company_id,event,start_events):\n",
    "    dataset.sort_values(by=[company_id,'new_time'])\n",
    "    return data.groupby(company_id).filter(lambda oneCompanyData: oneCompanyData.iloc[0][event] in start_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete traces that do not end with one of many end events\n",
    "# Need to sort\n",
    "\n",
    "def deleteTruncatedTracesEndSort(dataset,company_id,event,end_events):\n",
    "    dataset.sort_values(by=[company_id,'new_time'])\n",
    "    return data.groupby(company_id).filter(lambda oneCompanyData: oneCompanyData.iloc[-1][event] in end_events)\n",
    "# multiple conditions, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cce0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete traces that do not end with a specific end event\n",
    "# No need to sort\n",
    "\n",
    "def deleteTruncatedTracesEnd(dataset,company_id,event,end_events):\n",
    "    return data.groupby(company_id).filter(lambda oneCompanyData: oneCompanyData.iloc[-1][event] in end_events)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cea553fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete traces with total duration less than t\n",
    "def deleteTracesWithTimeLessSort(dataset,company_id,t):\n",
    "    dataset=dataset.sort_values(by=[company_id,'new_time'])\n",
    "    result=dataset.groupby(company_id).filter(lambda oneCompanyData: (oneCompanyData.iloc[-1].new_time - oneCompanyData.iloc[0].new_time) > t)\n",
    "    return result\n",
    "\n",
    "## format condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a5eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete traces with total duration less than t\n",
    "# Without sorting\n",
    "def deleteTracesWithTimeLessSort(dataset,company_id,t):\n",
    "    result=dataset.groupby(company_id).filter(lambda oneCompanyData: (oneCompanyData.iloc[-1].new_time - oneCompanyData.iloc[0].new_time) > t)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fe0358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the format of time\n",
    "def addNewTime(dataset,time,formats):\n",
    "    dataset['new_time']=pd.to_datetime(dataset[time],format=formats)\n",
    "    dataset['new_time']=(dataset['new_time'].dt.hour*60+dataset['new_time'].dt.minute)*60 + dataset['new_time'].dt.second\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8d44d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate two columns\n",
    "\n",
    "# data[\"test\"] = data[\"event\"].astype(str) + data[\"theme\"].astype(str)\n",
    "# Add separator\n",
    "\n",
    "def concatenateColumns(dataset,newCol,separator,*cols):\n",
    "    dataset[newCol]=\"\"\n",
    "    for col in cols:\n",
    "        dataset[newCol]=dataset[newCol]+separator+dataset[col].astype(str)\n",
    "    return  dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60cf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the keep parameter is set to False, so that only Unique values are taken and the duplicate values are removed from data.\n",
    "# Determines which duplicates (if any) to mark.\n",
    "# first : Mark duplicates as True except for the first occurrence.\n",
    "# last : Mark duplicates as True except for the last occurrence.\n",
    "# False : Mark all duplicates as True.\n",
    "## Group by id\n",
    "def eventIsRepeated(dataset,company_id,event):\n",
    "    res = pd.DataFrame([])\n",
    "    grouped = dataset.groupby(company_id)\n",
    "    for name, group in grouped:\n",
    "        group['match1'] = group.event.eq(group.event.shift()) \n",
    "        group['match2'] = group.event.eq(group.event.shift(-1)) \n",
    "        group[\"isRepeated\"] = group.apply(lambda x: x['match1'] or x['match2'], axis = 1)\n",
    "        group = group.drop([\"match1\", \"match2\"], axis = 1)\n",
    "        res = pd.concat([res, group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8535014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example, the keep parameter is set to False, so that only Unique values are taken and the duplicate values are removed from data.\n",
    "# Determines which duplicates (if any) to mark.\n",
    "# first : Mark duplicates as True except for the first occurrence.\n",
    "# last : Mark duplicates as True except for the last occurrence.\n",
    "# False : Mark all duplicates as True.\n",
    "## Group by id\n",
    "def eventIsRepeatedSort(dataset,company_id,event):\n",
    "    dataset.sort_values(by=[company_id,'new_time'])\n",
    "    res = pd.DataFrame([])\n",
    "    grouped = dataset.groupby(company_id)\n",
    "    for name, group in grouped:\n",
    "        group['match1'] = group.event.eq(group.event.shift()) \n",
    "        group['match2'] = group.event.eq(group.event.shift(-1)) \n",
    "        group[\"isRepeated\"] = group.apply(lambda x: x['match1'] or x['match2'], axis = 1)\n",
    "        group = group.drop([\"match1\", \"match2\"], axis = 1)\n",
    "        res = pd.concat([res, group])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb4a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep first event in each sequence of consecutive events in each trace of logs\n",
    "def keepFirstEventSort(dataset,event,company_id):\n",
    "    dataset=dataset.sort_values(by=[company_id,'new_time'])\n",
    "    res = pd.DataFrame([])\n",
    "    grouped2 = dataset.groupby(company_id)\n",
    "    for name, group in grouped2:\n",
    "        test = group.loc[group[event].ne(group[event].shift())]\n",
    "        res = pd.concat([res, test])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2196dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep first event in each sequence of consecutive events in each trace of logs\n",
    "def keepFirstEvent(dataset,event,company_id):\n",
    "    res = pd.DataFrame([])\n",
    "    grouped2 = dataset.groupby(company_id)\n",
    "    for name, group in grouped2:\n",
    "        test = group.loc[group[event].ne(group[event].shift())]\n",
    "        res = pd.concat([res, test])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb1a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep last event in each sequence of consecutive events in each trace of logs\n",
    "def keepLastEventSort(dataset,event,company_id):\n",
    "    dataset=dataset.sort_values(by=[company_id,'new_time'],ascending=False)\n",
    "    res = pd.DataFrame([])\n",
    "    grouped2 = dataset.groupby(company_id)\n",
    "    for name, group in grouped2:\n",
    "        test = group.loc[group[event].ne(group[event].shift())]\n",
    "        res = pd.concat([res, test])\n",
    "    return res.sort_values(by=[company_id,'new_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9096d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep last event in each sequence of consecutive events in each trace of logs\n",
    "def keepLastEvent(dataset,event,company_id,time):\n",
    "    res = pd.DataFrame([])\n",
    "    grouped2 = dataset.groupby(company_id)\n",
    "    for name, group in grouped2:\n",
    "        test = group.loc[group[event].ne(group[event].shift())]\n",
    "        res = pd.concat([res, test])\n",
    "    return res.sort_values(by=[company_id,time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5bf27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all events\n",
    "def deleteAllEvents(dataset,events,eventName):\n",
    "    return dataset.drop(dataset[dataset[events]==eventName].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge rows no sort\n",
    "def MergeSameEventRows(dataset,company_id,event_name,conditions):\n",
    "    res = pd.DataFrame([])\n",
    "    grouped = dataset.groupby(company_id)\n",
    "    for name, group in grouped:\n",
    "        group['sup'] = group.event.eq(group.event.shift()).map(lambda x: 0 if x == True else 1 ).cumsum()\n",
    "        res = pd.concat([res, group])\n",
    "    \n",
    "    lists = data.columns.to_list()\n",
    "    ignore = [company_id]\n",
    "    for column in lists:\n",
    "        if column not in ignore and column not in conditions:\n",
    "            conditions[column] = \"first\"\n",
    "    test = res.groupby([company_id, \"sup\"]).agg(conditions).reset_index().drop([\"sup\"], axis=1)\n",
    "    return test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b419b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge rows sort\n",
    "def MergeSameEventRowsSort(dataset,company_id,event_name,conditions):\n",
    "    dataset.sort_values(by=[company_id,'new_time'])\n",
    "    res = pd.DataFrame([])\n",
    "    grouped = dataset.groupby(company_id)\n",
    "    for name, group in grouped:\n",
    "        group['sup'] = group.event.eq(group.event.shift()).map(lambda x: 0 if x == True else 1 ).cumsum()\n",
    "        res = pd.concat([res, group])\n",
    "    \n",
    "    lists = data.columns.to_list()\n",
    "    ignore = [company_id]\n",
    "    for column in lists:\n",
    "        if column not in ignore and column not in conditions:\n",
    "            conditions[column] = \"first\"\n",
    "    result = res.groupby([company_id, \"sup\"]).agg(conditions).reset_index().drop([\"sup\"], axis=1)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53307774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ArrangeRows(dataset,condition):\n",
    "    return data.sort_values(by=condition)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
